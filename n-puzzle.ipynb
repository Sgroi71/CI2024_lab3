{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB3 - N-Puzzle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from random import choice\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import itertools\n",
    "from queue import PriorityQueue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUZZLE_DIM = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action:\n",
    "    def __init__(self, pos1, pos2):\n",
    "        self.pos1 = pos1\n",
    "        self.pos2 = pos2\n",
    "    def __str__(self):\n",
    "        return f\"({self.pos1}, {self.pos2})\"\n",
    "class Parent:\n",
    "    def __init__(self, state, action, parent):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.parent = parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_actions(state: np.ndarray) -> list['Action']:\n",
    "    x, y = [int(index[0]) for index in np.where(state == 0)]#return the position of the empty tile\n",
    "    actions = list()\n",
    "    if x > 0:\n",
    "        actions.append(Action((x, y), (x - 1, y)))\n",
    "    if x < PUZZLE_DIM - 1:\n",
    "        actions.append(Action((x, y), (x + 1, y)))\n",
    "    if y > 0:\n",
    "        actions.append(Action((x, y), (x, y - 1)))\n",
    "    if y < PUZZLE_DIM - 1:\n",
    "        actions.append(Action((x, y), (x, y + 1)))\n",
    "    return actions\n",
    "\n",
    "def do_action(state: np.ndarray, action: Action) -> np.ndarray:\n",
    "    new_state = state.copy()\n",
    "    new_state[action.pos1], new_state[action.pos2] = new_state[action.pos2], new_state[action.pos1]\n",
    "    return new_state\n",
    "\n",
    "def heuristic_simple(state: np.ndarray, goal_state:np.ndarray) -> int:\n",
    "    return int(np.sum(state != goal_state))\n",
    "\n",
    "def enhance_heuristic_manhattan(state: np.ndarray, goal_state:np.ndarray) -> int:\n",
    "    #manhattan distance\n",
    "    size = state.shape[0]\n",
    "    total_distance = 0\n",
    "    for row in range(size):\n",
    "        for col in range(size):\n",
    "            value = state[row, col]\n",
    "            if value != 0:  # Skip the empty tile\n",
    "                goal_row, goal_col = np.where(goal_state == value)\n",
    "                total_distance += abs(row - goal_row[0]) + abs(col - goal_col[0])\n",
    "    # 2*linear conflict because it is at least 2 moves to resolve a linear conflict\n",
    "    return total_distance + 2*linear_conflict(state, goal_state)\n",
    "\n",
    "\n",
    "def linear_conflict(puzzle, goal):\n",
    "    size = puzzle.shape[0]\n",
    "    total_conflicts = 0\n",
    "\n",
    "    def count_conflicts(line, goal_line):\n",
    "        conflicts = 0\n",
    "        for i in range(len(line)):\n",
    "            for j in range(i + 1, len(line)):\n",
    "                # for each pair of tiles in the same row/column\n",
    "                if line[i] in goal_line and line[j] in goal_line:\n",
    "                    # find their goal positions\n",
    "                    goal_i = np.where(goal_line == line[i])[0][0]\n",
    "                    goal_j = np.where(goal_line == line[j])[0][0]\n",
    "                    if goal_i > goal_j:  # they are in reverse order w.r.t goal state, so they are blocking each\n",
    "                        conflicts += 1\n",
    "        return conflicts\n",
    "\n",
    "    # Check rows for linear conflicts\n",
    "    for row in range(size):\n",
    "        total_conflicts += count_conflicts(puzzle[row, :], goal[row, :])\n",
    "\n",
    "    # Check columns for linear conflicts\n",
    "    for col in range(size):\n",
    "        total_conflicts += count_conflicts(puzzle[:, col], goal[:, col])\n",
    "\n",
    "    return total_conflicts\n",
    "def reconstruct_path(node: Parent):\n",
    "    path = []\n",
    "    act=None\n",
    "    while node is not None:\n",
    "        path.append((node.state,act))\n",
    "        act=node.action\n",
    "        node = node.parent\n",
    "    return path[::-1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A_star_search(start_state: np.ndarray, goal_state: np.ndarray):\n",
    "    evaluated_actions = 0\n",
    "\n",
    "    # Initialize frontier and visited set\n",
    "    visited = {}\n",
    "    \n",
    "    goal_tuple = tuple(map(tuple, goal_state))# Convert states to immutable tuples to use them as keys in the visited set\n",
    "    counter = itertools.count() # Counter to break ties in the priority queue\n",
    "    frontier = PriorityQueue()\n",
    "    frontier.put((0, next(counter),start_state, 0, None, None))  # Priority, state, g, last_action, parent\n",
    "\n",
    "    while not frontier.empty():\n",
    "        _, _, current_state, g, last_action, parent = frontier.get()\n",
    "\n",
    "        current_tuple = tuple(map(tuple, current_state))\n",
    "        if current_tuple == goal_tuple:\n",
    "            return reconstruct_path(Parent(current_state, last_action, parent)),evaluated_actions\n",
    "\n",
    "        if current_tuple not in visited:\n",
    "            visited[current_tuple] = g # Save the cost to reach the current state in order to avoid revisiting it in the future\n",
    "            for action in available_actions(current_state):\n",
    "                evaluated_actions += 1\n",
    "                new_state = do_action(current_state, action)\n",
    "                new_tuple = tuple(map(tuple, new_state))\n",
    "                if new_tuple not in visited or visited[current_tuple] > g+1:\n",
    "                    f = g + 1 + enhance_heuristic_manhattan(new_state, goal_state)\n",
    "                    frontier.put((f, next(counter),new_state, g + 1, action, Parent(current_state, last_action, parent)))\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Randomizing: 100%|██████████| 1000/1000 [00:00<00:00, 192744.08it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "RANDOMIZE_STEPS = 1000\n",
    "goal_state = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "state_init = goal_state.copy()\n",
    "for r in tqdm(range(RANDOMIZE_STEPS), desc='Randomizing'):\n",
    "    state_init = do_action(state_init, choice(available_actions(state_init)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A* Search Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3 4]\n",
      " [6 8 7]\n",
      " [0 1 5]]->((2, 0), (2, 1))\n",
      "[[2 3 4]\n",
      " [6 8 7]\n",
      " [1 0 5]]->((2, 1), (1, 1))\n",
      "[[2 3 4]\n",
      " [6 0 7]\n",
      " [1 8 5]]->((1, 1), (1, 0))\n",
      "[[2 3 4]\n",
      " [0 6 7]\n",
      " [1 8 5]]->((1, 0), (2, 0))\n",
      "[[2 3 4]\n",
      " [1 6 7]\n",
      " [0 8 5]]->((2, 0), (2, 1))\n",
      "[[2 3 4]\n",
      " [1 6 7]\n",
      " [8 0 5]]->((2, 1), (1, 1))\n",
      "[[2 3 4]\n",
      " [1 0 7]\n",
      " [8 6 5]]->((1, 1), (1, 2))\n",
      "[[2 3 4]\n",
      " [1 7 0]\n",
      " [8 6 5]]->((1, 2), (0, 2))\n",
      "[[2 3 0]\n",
      " [1 7 4]\n",
      " [8 6 5]]->((0, 2), (0, 1))\n",
      "[[2 0 3]\n",
      " [1 7 4]\n",
      " [8 6 5]]->((0, 1), (0, 0))\n",
      "[[0 2 3]\n",
      " [1 7 4]\n",
      " [8 6 5]]->((0, 0), (1, 0))\n",
      "[[1 2 3]\n",
      " [0 7 4]\n",
      " [8 6 5]]->((1, 0), (1, 1))\n",
      "[[1 2 3]\n",
      " [7 0 4]\n",
      " [8 6 5]]->((1, 1), (1, 2))\n",
      "[[1 2 3]\n",
      " [7 4 0]\n",
      " [8 6 5]]->((1, 2), (2, 2))\n",
      "[[1 2 3]\n",
      " [7 4 5]\n",
      " [8 6 0]]->((2, 2), (2, 1))\n",
      "[[1 2 3]\n",
      " [7 4 5]\n",
      " [8 0 6]]->((2, 1), (2, 0))\n",
      "[[1 2 3]\n",
      " [7 4 5]\n",
      " [0 8 6]]->((2, 0), (1, 0))\n",
      "[[1 2 3]\n",
      " [0 4 5]\n",
      " [7 8 6]]->((1, 0), (1, 1))\n",
      "[[1 2 3]\n",
      " [4 0 5]\n",
      " [7 8 6]]->((1, 1), (1, 2))\n",
      "[[1 2 3]\n",
      " [4 5 0]\n",
      " [7 8 6]]->((1, 2), (2, 2))\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 0]]->None\n",
      "quality: 21\n",
      "cost: 509\n",
      "time: 0.04483318328857422s\n"
     ]
    }
   ],
   "source": [
    "# count the time in milliseconds\n",
    "import time\n",
    "start_time = time.time()\n",
    "path, cost= A_star_search(state_init, goal_state)\n",
    "final_time = (time.time() - start_time)\n",
    "\n",
    "if path is None:\n",
    "    print(\"No solution found\")\n",
    "else:\n",
    "    for state, action in path:\n",
    "        print(f\"{state}->{action}\")\n",
    "    print(f\"quality: {len(path)}\")\n",
    "    print(f\"cost: {cost}\")\n",
    "    print(f\"time: {final_time}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-WEKR9SVn-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
